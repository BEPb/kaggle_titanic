{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":11.015258,"end_time":"2023-01-08T19:45:12.828135","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-08T19:45:01.812877","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.postimg.cc/nMbJLmwc/Screenshot-3.png)","metadata":{"papermill":{"duration":0.002867,"end_time":"2023-01-08T19:45:10.405606","exception":false,"start_time":"2023-01-08T19:45:10.402739","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\nPython 3.10 LassoLars program with pre-processing of cagle titanic competition data\nFile name: LassoLars.py\n\nVersion: 0.1\nAuthor: Andrej Marinchenko\nDate: 2023-01-08\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Read in the training and test sets\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n# print(len(train_df))\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n# print(len(test_df))\nresult_df = pd.read_csv('/kaggle/input/titanic-competition-how-top-lb-got-their-score/submission.csv')   # 100% result\n# print(len(result_df))\n\n###################################### Preprocess the data #############################################################\n# Identify most relevant features\n# You can use techniques like feature importance or correlation analysis to help you identify the most important features\nrelevant_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n# Handle missing values\nimputer = SimpleImputer(strategy='most_frequent')\ntrain_df[relevant_features] = imputer.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = imputer.transform(test_df[relevant_features])\n\n# Encode categorical variables as numeric\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n\n# Transform skewed or non-normal features\n# Instead of normalizing all of the numeric features, you could try using techniques like log transformation or\n# Box-Cox transformation to make the distribution of a feature more normal\nscaler = StandardScaler()\ntrain_df[relevant_features] = scaler.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = scaler.transform(test_df[relevant_features])\n\n# Split the data into features (X) and labels (y)\nX_train = train_df[relevant_features]\ny_train = train_df['Survived']\nX_test = test_df[relevant_features]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=33)\n\n\n############################################## Train the model #########################################################\nmodel = linear_model.LassoLars()\nmodel.fit(X_train, y_train)\n\n# Evaluate the logistic regression classifier\nscores = cross_val_score(model, X_val, y_val, cv=5)\nprint(\"Accuracy of linear regression classifier: \", scores.mean())\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred})\noutput['Survived']= output['Survived'].astype(int)\noutput.to_csv('submission.csv', index=False)\n\n# print(output)\nprint('Correlation with ideal submission:', output['Survived'].corr(result_df['Survived']))\nprint('Real score on submission: 0.0')\n\n\n# The coefficients\nprint(\"Coefficients: \\n\",  model.coef_)\n\n# Plot outputs\n# plt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred, color=\"blue\", linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.691701,"end_time":"2023-01-08T19:45:12.099275","exception":false,"start_time":"2023-01-08T19:45:10.407574","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-01-08T19:52:45.848197Z","iopub.execute_input":"2023-01-08T19:52:45.848620Z","iopub.status.idle":"2023-01-08T19:52:46.012011Z","shell.execute_reply.started":"2023-01-08T19:52:45.848585Z","shell.execute_reply":"2023-01-08T19:52:46.004384Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Accuracy of linear regression classifier:  -0.016605957049362408\nCorrelation with ideal submission: nan\nReal score on submission: 0.0\nCoefficients: \n [0. 0. 0. 0. 0. 0. 0.]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\nSet parameter alpha to: original_alpha * np.sqrt(n_samples). \n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAADsElEQVR4nO3ZMU7kMBiAUQdxhKFJs7n/WWYOQQEUcAdvj7bYkZJ8YnivtCL7rz5ZzjLnHACc76keAOC3EmCAiAADRAQYICLAABEBBog83/Px5XKZ27YdNArAY7rdbl9zzpfv63cFeNu2cb1e95sK4BdYluX1X+ueIAAiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQeT7jkHUd4+PjjJMAjrGuY7y97bvnKTdg8QV+uvf3/fc8JcDresYpAMc5omOnPEHsfW0HeAR+wgFEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEljnn/3+8LJ9jjNfjxgF4SH/mnC/fF+8KMAD78QQBEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEPkLwO4lR6W6iXoAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### If you liked this core, you can also check out my other [works](https://www.kaggle.com/marinchenko/notebooks) and [databases](https://www.kaggle.com/marinchenko/datasets).\n#### I am looking for friends to develop as a machine learning specialist. \n#### Open to dialogue and criticism. \n### Thank you for your time!","metadata":{"papermill":{"duration":0.002516,"end_time":"2023-01-08T19:45:12.104617","exception":false,"start_time":"2023-01-08T19:45:12.102101","status":"completed"},"tags":[]}}]}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":10.244191,"end_time":"2023-01-08T16:14:58.544356","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-08T16:14:48.300165","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.postimg.cc/nMbJLmwc/Screenshot-3.png)","metadata":{"papermill":{"duration":0.002604,"end_time":"2023-01-08T16:14:56.355997","exception":false,"start_time":"2023-01-08T16:14:56.353393","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\nPython 3.10 Lasso program with pre-processing of cagle titanic competition data\nFile name: Lasso.py\n\nVersion: 0.1\nAuthor: Andrej Marinchenko\nDate: 2023-01-08\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Read in the training and test sets\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n# print(len(train_df))\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n# print(len(test_df))\nresult_df = pd.read_csv('/kaggle/input/titanic-competition-how-top-lb-got-their-score/submission.csv')   # 100% result\n# print(len(result_df))\n\n###################################### Preprocess the data #############################################################\n# Identify most relevant features\n# You can use techniques like feature importance or correlation analysis to help you identify the most important features\nrelevant_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n# Handle missing values\nimputer = SimpleImputer(strategy='most_frequent')\ntrain_df[relevant_features] = imputer.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = imputer.transform(test_df[relevant_features])\n\n# Encode categorical variables as numeric\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n\n# Transform skewed or non-normal features\n# Instead of normalizing all of the numeric features, you could try using techniques like log transformation or\n# Box-Cox transformation to make the distribution of a feature more normal\nscaler = StandardScaler()\ntrain_df[relevant_features] = scaler.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = scaler.transform(test_df[relevant_features])\n\n# Split the data into features (X) and labels (y)\nX_train = train_df[relevant_features]\ny_train = train_df['Survived']\nX_test = test_df[relevant_features]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=33)\n\n\n############################################## Train the model #########################################################\nmodel = linear_model.Lasso()\nmodel.fit(X_train, y_train)\n\n# Evaluate the logistic regression classifier\nscores = cross_val_score(model, X_val, y_val, cv=5)\nprint(\"Accuracy of linear regression classifier: \", scores.mean())\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred})\noutput['Survived']= output['Survived'].astype(int)\noutput.to_csv('submission.csv', index=False)\n\n# print(output)\nprint('Correlation with ideal submission:', output['Survived'].corr(result_df['Survived']))\nprint('Real score on submission: 0.622')\n\n\n# The coefficients\nprint(\"Coefficients: \\n\",  model.coef_)\n\n# Plot outputs\n# plt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred, color=\"blue\", linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.459708,"end_time":"2023-01-08T16:14:57.817526","exception":false,"start_time":"2023-01-08T16:14:56.357818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-01-08T19:12:36.420034Z","iopub.execute_input":"2023-01-08T19:12:36.420902Z","iopub.status.idle":"2023-01-08T19:12:38.006346Z","shell.execute_reply.started":"2023-01-08T19:12:36.420801Z","shell.execute_reply":"2023-01-08T19:12:38.004633Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Accuracy of linear regression classifier:  -0.016605957049362408\n     PassengerId  Survived\n0            892         0\n1            893         0\n2            894         0\n3            895         0\n4            896         0\n..           ...       ...\n413         1305         0\n414         1306         0\n415         1307         0\n416         1308         0\n417         1309         0\n\n[418 rows x 2 columns]\nCorrelation with ideal submission: nan\nReal score on submission: 0.65311\nCoefficients: \n [-0.  0. -0. -0.  0.  0.  0.]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAADsElEQVR4nO3ZMU7kMBiAUQdxhKFJs7n/WWYOQQEUcAdvj7bYkZJ8YnivtCL7rz5ZzjLnHACc76keAOC3EmCAiAADRAQYICLAABEBBog83/Px5XKZ27YdNArAY7rdbl9zzpfv63cFeNu2cb1e95sK4BdYluX1X+ueIAAiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQeT7jkHUd4+PjjJMAjrGuY7y97bvnKTdg8QV+uvf3/fc8JcDresYpAMc5omOnPEHsfW0HeAR+wgFEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEljnn/3+8LJ9jjNfjxgF4SH/mnC/fF+8KMAD78QQBEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEPkLwO4lR6W6iXoAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### If you liked this core, you can also check out my other [works](https://www.kaggle.com/marinchenko/notebooks) and [databases](https://www.kaggle.com/marinchenko/datasets).\n#### I am looking for friends to develop as a machine learning specialist. \n#### Open to dialogue and criticism. \n### Thank you for your time!","metadata":{"papermill":{"duration":0.001566,"end_time":"2023-01-08T16:14:57.821025","exception":false,"start_time":"2023-01-08T16:14:57.819459","status":"completed"},"tags":[]}}]}
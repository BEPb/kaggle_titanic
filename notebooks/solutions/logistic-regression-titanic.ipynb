{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# Read in the training and test sets\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n###################################### Preprocess the data ###################################################################\n# Identify most relevant features\n# You can use techniques like feature importance or correlation analysis to help you identify the most important features\nrelevant_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\n# Handle missing values\nimputer = SimpleImputer(strategy='most_frequent')\ntrain_df[relevant_features] = imputer.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = imputer.transform(test_df[relevant_features])\n\n# Encode categorical variables as numeric\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n\n# Transform skewed or non-normal features\n# Instead of normalizing all of the numeric features, you could try using techniques like log transformation or Box-Cox transformation to make the distribution of a feature more normal\nscaler = StandardScaler()\ntrain_df[relevant_features] = scaler.fit_transform(train_df[relevant_features])\ntest_df[relevant_features] = scaler.transform(test_df[relevant_features])\n\n# Split the data into features (X) and labels (y)\nX_train = train_df[relevant_features]\ny_train = train_df['Survived']\nX_test = test_df[relevant_features]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=33)\n\n\n############################################## Train the model ################################################################\nlog_reg = LogisticRegression()\nmodel = VotingClassifier(estimators=[('lr', log_reg)])\nmodel.fit(X_train, y_train)\n\n\nparam_grid = {'C': [0.1, 1, 10]}\ngrid_search = GridSearchCV(log_reg, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_log_reg = grid_search.best_estimator_\nprint(\"Best parameters for logistic regression: \", grid_search.best_params_)\n\n\nmodel = VotingClassifier(estimators=[('lr', best_log_reg)])\nmodel.fit(X_train, y_train)\n\n# Evaluate the fine-tuned model\ny_pred = model.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Evaluate the logistic regression classifier\nscores = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint(\"Accuracy of logistic regression classifier: \", scores.mean())\n\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}